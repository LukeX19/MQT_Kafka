1. Create topic a topic having 3 brokers as --bootstrap-server. 
The brokers' ports are taken from the docker-compose_kafka.yml
The topic should have 3 partitions and replication-factor 3
The name of the topic is events1. 

	/usr/bin/kafka-topics --create --bootstrap-server kafka:9092,kafka2:9093,kafka3:9094 --partitions 3 --replication-factor 3 --topic events1

    
Create a second topic having 3 brokers as --bootstrap-server. 
The brokers' ports are taken from the docker-compose_kafka.yml
The topic should have 4 partitions and replication-factor 2
The name of the topic is events2. 	

	Using 9092, 9093 and 9094 ports generated the error:
		[2023-10-07 17:30:09,017] WARN [Producer clientId=console-producer] Connection to node 2 (localhost/127.0.0.1:9093) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
	Tried using ports 19092, 19093 and 19094 (the "LISTENER_DOCKER_INTERNAL" ports, from the yml file):
		/usr/bin/kafka-topics --create --bootstrap-server kafka:9092,kafka2:9093,kafka3:9094 --partitions 4 --replication-factor 2 --topic events2


2. List all topics 

	/usr/bin/kafka-topics --list --bootstrap-server kafka:19092,kafka2:19093,kafka3:19094


3. Send data. Create a Producer and send data to events2 topic, using key and ',' as separator.

	/usr/bin/kafka-console-producer --bootstrap-server kafka:19092,kafka2:19093,kafka3:19094 --topic events2 --property parse.key=true --property key.separator=','


3. Read the data. Create a Consumer and read data from events2 topic.  
No data will be shown because we do not have --from-beginning property

	Without printing the key:
		docker exec -ti kafka /usr/bin/kafka-console-consumer --bootstrap-server kafka:19092,kafka2:19093,kafka3:19094 --topic events2
	With printing of the key:
		docker exec -ti kafka /usr/bin/kafka-console-consumer --bootstrap-server kafka:19092,kafka2:19093,kafka3:19094 --topic events2 --property print.key=true


4. Run consumer and print the partition
Check that messages with the same key go to the same partition. Notice, that messages may come in a different order, when they are in different partitions.

	docker exec -ti kafka /usr/bin/kafka-console-consumer --bootstrap-server kafka:19092,kafka2:19093,kafka3:19094 --topic events2 --property print.key=true --property print.partition=true


5. Run with specifying consumer group and printing the partition and the offset
Check that messages with the same key go to the same partition. Notice, that messages may come in a different order, when they are in different partitions.

	docker exec -ti kafka /usr/bin/kafka-console-consumer --bootstrap-server kafka:19092,kafka2:19093,kafka3:19094 --topic events2 --property print.key=true --property print.partition=true --property print.offset=true


6. Create a group of 2 consumers. 
Now we have two consumers within one group.

	docker exec -ti kafka /usr/bin/kafka-console-consumer --bootstrap-server kafka:19092,kafka2:19093,kafka3:19094 --topic events2 --group 1 --property print.key=true --property print.partition=true --property print.offset=true


7. Send more data with our producer
Check how the messages are distributed between the 2 consumers. 

	One consumer takes responsibility for the first two partitions (Partition 0 and Partition 1), while the second consumer takes the remaining two partitions (Partition 2 and Partition 3).


8. Delete topic

	/usr/bin/kafka-topics --bootstrap-server kafka:19092,kafka2:19093,kafka3:19094 --topic events2 --delete

